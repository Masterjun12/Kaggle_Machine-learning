{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBjdxno_4Gjd"
   },
   "source": [
    "### 목적 : pacmap 과 iforest 를 통한 이상치 탐지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEEoDfompF3b",
    "outputId": "ee3655fb-02da-497a-eb2e-d3f9ae0fd757"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "# 구글 드라이브 입력\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J98RvVnMq771",
    "outputId": "d3abee11-7c5d-4506-8517-e2b5a042419a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.7/647.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install -q annoy\n",
    "! pip install -q FRUFS\n",
    "! pip install -q pacmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "7pULCAakpcuZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import pacmap\n",
    "\n",
    "from FRUFS import FRUFS\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from scipy.stats import ranksums\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# 파일 위치 고정\n",
    "os.chdir(\"/content/gdrive/MyDrive/open\")\n",
    "\n",
    "# Train dataset\n",
    "train_df = pd.read_csv('train.csv')\n",
    "train_df = train_df.iloc[:,1:]\n",
    "\n",
    "# Validation dataset\n",
    "val_df = pd.read_csv('val.csv')\n",
    "ori_val_df = val_df.iloc[:,1:]\n",
    "val_class = val_df.iloc[:,31]\n",
    "val_df = val_df.iloc[:,1:31]\n",
    "\n",
    "# Test dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df = test_df.iloc[:,1:]\n",
    "\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9v-3MVsVlGI"
   },
   "source": [
    "## 함수정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fYaK5Zib1mii"
   },
   "outputs": [],
   "source": [
    "# IsolationForest 모델 출력 (1:정상, -1:불량(사기)) 이므로 (0:정상, 1:불량(사기))로 Label 변환\n",
    "def get_pred_label(model_pred):\n",
    "    model_pred = np.where(model_pred == 1, 0, model_pred)\n",
    "    model_pred = np.where(model_pred == -1, 1, model_pred)\n",
    "    return model_pred\n",
    "\n",
    "# IsolationForest 예측만 하는 함수\n",
    "def iso_for_model_prediction(the_contamination, trtrtr):\n",
    "    # train dataset으로 isolationforest 모델 학습\n",
    "    model_only_train = IsolationForest(n_estimators=300, contamination=the_contamination, verbose=0)\n",
    "    model_only_train.fit(trtrtr)\n",
    "\n",
    "    # train dataset의 isolationforest 모델로 예측\n",
    "    train_pred = model_only_train.predict(trtrtr)\n",
    "    train_pred = get_pred_label(train_pred)\n",
    "\n",
    "    return train_pred\n",
    "\n",
    "# Pacmac + IsolationForest 예측과 비교할 수 있는 함수\n",
    "def pac_iso_for_model_comparing(the_contamination, trtrtr, low_dim, compare_class):\n",
    "    # train dataset으로 isolationforest 모델 학습\n",
    "    dlatl_embedding = pacmap.PaCMAP(n_components=low_dim, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 100, verbose = True)\n",
    "    pac_mac_train = dlatl_embedding.fit_transform(np.array(trtrtr), init=\"pca\")\n",
    "\n",
    "    model_train_compare = IsolationForest(n_estimators=300, contamination=the_contamination, verbose=0)\n",
    "    model_train_compare.fit(pac_mac_train)\n",
    "\n",
    "    # train dataset의 isolationforest 모델로 예측\n",
    "    train_pred = model_train_compare.predict(pac_mac_train)\n",
    "    train_pred = get_pred_label(train_pred)\n",
    "\n",
    "    # train dataset의 예측치와 compare data의 수치 비교\n",
    "    train_score = f1_score(compare_class, train_pred, average='macro')\n",
    "\n",
    "    print(f'Compared Macro F1 Score : [{train_score}]')\n",
    "    print(classification_report(compare_class, train_score))\n",
    "\n",
    "# IQR Method에서 경계값을 나타낸 함수\n",
    "def iqr_outlier(ddff):\n",
    "    q1 = ddff.quantile(0.25)\n",
    "    q3 = ddff.quantile(0.75)\n",
    "\n",
    "    iqr = q3 - q1\n",
    "\n",
    "    lower_bound = q1 - (1.5 * iqr)\n",
    "    upper_bound = q3 + (1.5 * iqr)\n",
    "\n",
    "    return pd.concat([lower_bound, upper_bound], axis= 1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4x63odr-gZ0q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6OjAXiT3q4L"
   },
   "source": [
    "## val 데이터에서 1차 변수 선택을 진행\n",
    "\n",
    "- val dataset 의 outlier들의 중앙값 계산\n",
    "\n",
    "- 이 중앙값이 IQR 경계값 사이에 있으면 drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xKld2opgreZg",
    "outputId": "1c826f9c-9938-4631-d0b1-6d3db93fa052"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 6, 8, 9, 10, 11, 13, 15, 16, 17]\n"
     ]
    }
   ],
   "source": [
    "## 변수 선택(1)\n",
    "def first_variation_selection(dfdfdf):\n",
    "    # Validation dataset의 outlier들의 중앙값이 Validation dataset의 IQR Method에서 경계값 사이에 있으면 이상치를 판단하지 못하는 변수\n",
    "    # class 열도 있으므로 1개 제외\n",
    "    how_many_var = (len(dfdfdf.columns) - 1)\n",
    "    new_var = []\n",
    "\n",
    "    for what_val in range(how_many_var):\n",
    "        if (iqr_outlier(dfdfdf).iloc[0,what_val] < dfdfdf.iloc[np.where(dfdfdf['Class'] == 1)].quantile(0.5)[what_val] < iqr_outlier(dfdfdf).iloc[1,what_val]):\n",
    "            continue\n",
    "        else:\n",
    "            new_var.append(what_val)\n",
    "\n",
    "    return new_var\n",
    "\n",
    "superior_var1 = first_variation_selection(ori_val_df)\n",
    "print(superior_var1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cq8Z3-P-4g1M"
   },
   "source": [
    "## 2차 변수 선정\n",
    "\n",
    "- Wilcoxon rank-sum test: 각 변수에 대해 사기(transactions with Class=1)와 정상(transactions with Class=0) 그룹 간의 중앙값의 차이를 검정하여 p-value를 계산 p-value가 낮은 순서대로 가장 유의한 차이가 있는 변수\n",
    "\n",
    "- LightGBM을 사용하여 회귀 모델을 학습하고, 학습된 모델의 특성 중요도를 계산\n",
    "\n",
    "- 두 가지 방법으로 선택된 변수를 합집합화하여 최종 변수 집합을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiCo9p4Ixcj6",
    "outputId": "31584c94-f4aa-4274-a7a0-9673c39cb511"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V10', 'V14', 'V11', 'V4', 'V12']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['V10', 'V4', 'V14', 'V17', 'V16']\n",
      "[3, 9, 10, 11, 13, 15, 16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  12 out of  12 | elapsed:   30.2s finished\n"
     ]
    }
   ],
   "source": [
    "## 변수 선택(2)\n",
    "# 2차 변수 선택\n",
    "def second_variation_selection(dfdfdf, trtrtr, nnn_var):\n",
    "    ranksum_pval = []\n",
    "    for what_val in nnn_var:\n",
    "        ranksum_pval.append(ranksums(dfdfdf.iloc[np.where(dfdfdf['Class'] == 1)].iloc[:,what_val], dfdfdf.iloc[np.where(dfdfdf['Class'] == 0)].iloc[:,what_val]).pvalue)\n",
    "\n",
    "    Wilcoxon_rank_sum_pval_var = list(pd.DataFrame({'pval':ranksum_pval, 'col':dfdfdf.columns[nnn_var]}).sort_values(by=['pval']).iloc[range(5),1])\n",
    "\n",
    "    print(Wilcoxon_rank_sum_pval_var)\n",
    "\n",
    "    # FRUFS를 이용하여 변수 선택\n",
    "    # core가 많으면 n_jobs 조정을 하면 됨.\n",
    "    model_frufs = FRUFS(model_r=LGBMRegressor(random_state=28), k=5, n_jobs=-1, verbose=1, random_state=28)\n",
    "    df_train_pruned = model_frufs.fit_transform(trtrtr.iloc[:,nnn_var])\n",
    "    FRUFS_LGBMRegressor_var = list(df_train_pruned.columns)\n",
    "\n",
    "    #plt.figure(figsize=(5, 6), dpi=100)\n",
    "    #model_frufs.feature_importance()\n",
    "    print(FRUFS_LGBMRegressor_var)\n",
    "\n",
    "    # 종합\n",
    "    all_var_in_td = list(trtrtr.columns)\n",
    "    new_var_set = list(set(Wilcoxon_rank_sum_pval_var + FRUFS_LGBMRegressor_var))\n",
    "    new_var_list = []\n",
    "\n",
    "    for nvs in new_var_set:\n",
    "        new_var_list.append(all_var_in_td.index(nvs))\n",
    "\n",
    "    new_var_list.sort()\n",
    "    return new_var_list\n",
    "\n",
    "superior_var2 = second_variation_selection(ori_val_df, train_df, superior_var1)\n",
    "print(superior_var2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqCSnpjZCUs7",
    "outputId": "3d905fb5-222d-4788-a478-fdfed0ca013f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 4, 5, 6, 7, 8, 12, 14, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n"
     ]
    }
   ],
   "source": [
    "## 변수 선택(3)\n",
    "# 선택한 변수 제외 나머지 변수 모임\n",
    "inferior_var2 = [x for x in range(30) if x not in superior_var2]\n",
    "print(inferior_var2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Sncow6C473S"
   },
   "source": [
    ":validation set의 사기 거래 비율"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t2s3Iquh2gbo",
    "outputId": "cc0218b0-f504-4e79-d67e-ccf73c7c39c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation contamination:[0.0010551491277433877]\n"
     ]
    }
   ],
   "source": [
    "## validation set의 사기 거래 비율\n",
    "ori_val_normal, ori_val_fraud = ori_val_df['Class'].value_counts()\n",
    "ori_val_contamination = ori_val_fraud / ori_val_normal\n",
    "print(f'Validation contamination:[{ori_val_contamination}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R2T8mO5X5DN9"
   },
   "source": [
    "## 학습및 예측\n",
    "\n",
    "PacMap (PaCMAP):\n",
    "\n",
    "- 비선형 차원 축소 알고리즘\n",
    "- 주어진 고차원 데이터를 저차원 공간으로 투영\n",
    "- 데이터의 비선형 구조를 보존하려고 노력\n",
    "- PacMap은 주로 매니폴드 학습에 기반하여 작동,\n",
    "- 비슷한 패턴을 가진 포인터들은 가깝게 매핑\n",
    "- 다른 포인터들은 멀리 패밍\n",
    "\n",
    "\n",
    "Isolation Forest:\n",
    "\n",
    "- 이상치 탐지를 위한 알고리즘 중 하나\n",
    "- 데이터의 이상치를 식별하는 데 사용됨\n",
    "- 데이터를 재귀적으로 분할하는 결정 트리를 구성\n",
    "- 각 분할에서는 랜덤하게 선택한 특성과 임계값을 기준으로 데이터를 분할\n",
    "- 이상치는 일반적인 데이터보다 더 빨리 분리되기 때문에 더 짧은 경로를 거침\n",
    "- 따라서 이상치는 더 낮은 평균 경로 길이를 갖게 되어 이를 통해 식별됨\n",
    "\n",
    "\n",
    "1. PacMap을 이용한 데이터 변환:\n",
    "\n",
    "- n_components는 저차원 공간의 차원 수\n",
    "- n_neighbors는 이웃의 수\n",
    "- MN_ratio는 Mahalanobis 거리 계산 시 고려되는 이웃의 비율\n",
    "- FP_ratio는 거리 계산 시 고려되는 이웃의 비율\n",
    "- num_iters는 최적화 반복 횟수\n",
    "\n",
    "2. Isolation Forest를 이용한 이상치 탐지 및 예측:\n",
    "\n",
    "- n_estimators는 생성할 결정 트리의 개수\n",
    "- contamination은 이상치로 판단할 비율\n",
    "\n",
    "3. Voting을 통한 최종 예측 결정:\n",
    "\n",
    "1. 여러 번의 반복(hhmm 횟수)을 통해 얻은 예측 결과를 종합\n",
    "2. train 데이터와 validation 데이터, test 데이터에 대한 예측 결과를 평균내어 최종 예측 결과 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Muij_O8F7xE1",
    "outputId": "244319f7-2473-4431-e85b-09c0be706505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2403947.250000\n",
      "Iteration:   20, Loss: 2149014.500000\n",
      "Iteration:   30, Loss: 2015343.500000\n",
      "Iteration:   40, Loss: 1911323.125000\n",
      "Iteration:   50, Loss: 1812000.250000\n",
      "Iteration:   60, Loss: 1706690.000000\n",
      "Iteration:   70, Loss: 1588427.500000\n",
      "Iteration:   80, Loss: 1448268.375000\n",
      "Iteration:   90, Loss: 1271796.000000\n",
      "Iteration:  100, Loss: 1005818.000000\n",
      "Iteration:  110, Loss: 1316072.625000\n",
      "Iteration:  120, Loss: 1293769.500000\n",
      "Iteration:  130, Loss: 1284269.750000\n",
      "Iteration:  140, Loss: 1281094.000000\n",
      "Iteration:  150, Loss: 1280528.875000\n",
      "Iteration:  160, Loss: 1280529.750000\n",
      "Iteration:  170, Loss: 1280611.000000\n",
      "Iteration:  180, Loss: 1281315.750000\n",
      "Iteration:  190, Loss: 1282268.250000\n",
      "Iteration:  200, Loss: 1282487.750000\n",
      "Iteration:  210, Loss: 540431.250000\n",
      "Iteration:  220, Loss: 535349.125000\n",
      "Iteration:  230, Loss: 530743.375000\n",
      "Iteration:  240, Loss: 528347.812500\n",
      "Iteration:  250, Loss: 526381.750000\n",
      "Iteration:  260, Loss: 524688.750000\n",
      "Iteration:  270, Loss: 523336.000000\n",
      "Iteration:  280, Loss: 522359.937500\n",
      "Iteration:  290, Loss: 521454.937500\n",
      "Iteration:  300, Loss: 520527.968750\n",
      "Elapsed time: 209.32s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1268450.5\n",
      "Iteration:   10, Loss: 366185.437500\n",
      "Iteration:   20, Loss: 260032.000000\n",
      "Iteration:   30, Loss: 221337.140625\n",
      "Iteration:   40, Loss: 205201.843750\n",
      "Iteration:   50, Loss: 201227.062500\n",
      "Iteration:   60, Loss: 199446.812500\n",
      "Iteration:   70, Loss: 198805.234375\n",
      "Iteration:   80, Loss: 198563.437500\n",
      "Iteration:   90, Loss: 198494.250000\n",
      "Iteration:  100, Loss: 198456.031250\n",
      "Iteration:  110, Loss: 298333.156250\n",
      "Iteration:  120, Loss: 297998.812500\n",
      "Iteration:  130, Loss: 297890.031250\n",
      "Iteration:  140, Loss: 297856.937500\n",
      "Iteration:  150, Loss: 297833.875000\n",
      "Iteration:  160, Loss: 297823.000000\n",
      "Iteration:  170, Loss: 297825.125000\n",
      "Iteration:  180, Loss: 297850.156250\n",
      "Iteration:  190, Loss: 297866.031250\n",
      "Iteration:  200, Loss: 297882.343750\n",
      "Iteration:  210, Loss: 99271.273438\n",
      "Iteration:  220, Loss: 99232.953125\n",
      "Iteration:  230, Loss: 99225.734375\n",
      "Iteration:  240, Loss: 99222.664062\n",
      "Iteration:  250, Loss: 99221.820312\n",
      "Iteration:  260, Loss: 99221.445312\n",
      "Iteration:  270, Loss: 99221.406250\n",
      "Iteration:  280, Loss: 99221.406250\n",
      "Iteration:  290, Loss: 99221.242188\n",
      "Iteration:  300, Loss: 99221.195312\n",
      "Elapsed time: 0:00:12.601137\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6371019.5\n",
      "Iteration:   10, Loss: 1826754.875000\n",
      "Iteration:   20, Loss: 1299366.000000\n",
      "Iteration:   30, Loss: 1105803.250000\n",
      "Iteration:   40, Loss: 1016149.000000\n",
      "Iteration:   50, Loss: 1000137.125000\n",
      "Iteration:   60, Loss: 992966.062500\n",
      "Iteration:   70, Loss: 990668.250000\n",
      "Iteration:   80, Loss: 989867.250000\n",
      "Iteration:   90, Loss: 989606.125000\n",
      "Iteration:  100, Loss: 989488.500000\n",
      "Iteration:  110, Loss: 1473831.500000\n",
      "Iteration:  120, Loss: 1470675.375000\n",
      "Iteration:  130, Loss: 1469941.875000\n",
      "Iteration:  140, Loss: 1469789.250000\n",
      "Iteration:  150, Loss: 1469604.375000\n",
      "Iteration:  160, Loss: 1469602.875000\n",
      "Iteration:  170, Loss: 1469702.625000\n",
      "Iteration:  180, Loss: 1469770.125000\n",
      "Iteration:  190, Loss: 1469870.000000\n",
      "Iteration:  200, Loss: 1470046.625000\n",
      "Iteration:  210, Loss: 494877.937500\n",
      "Iteration:  220, Loss: 494748.562500\n",
      "Iteration:  230, Loss: 494729.968750\n",
      "Iteration:  240, Loss: 494715.562500\n",
      "Iteration:  250, Loss: 494712.750000\n",
      "Iteration:  260, Loss: 494710.062500\n",
      "Iteration:  270, Loss: 494710.468750\n",
      "Iteration:  280, Loss: 494709.812500\n",
      "Iteration:  290, Loss: 494710.125000\n",
      "Iteration:  300, Loss: 494710.156250\n",
      "Elapsed time: 0:00:51.594830\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2403130.500000\n",
      "Iteration:   20, Loss: 2148786.000000\n",
      "Iteration:   30, Loss: 2015052.250000\n",
      "Iteration:   40, Loss: 1911014.375000\n",
      "Iteration:   50, Loss: 1811822.000000\n",
      "Iteration:   60, Loss: 1706572.625000\n",
      "Iteration:   70, Loss: 1588360.000000\n",
      "Iteration:   80, Loss: 1448143.750000\n",
      "Iteration:   90, Loss: 1271731.000000\n",
      "Iteration:  100, Loss: 1005707.500000\n",
      "Iteration:  110, Loss: 1315628.750000\n",
      "Iteration:  120, Loss: 1293606.125000\n",
      "Iteration:  130, Loss: 1284138.125000\n",
      "Iteration:  140, Loss: 1280961.000000\n",
      "Iteration:  150, Loss: 1280429.250000\n",
      "Iteration:  160, Loss: 1280555.000000\n",
      "Iteration:  170, Loss: 1280527.250000\n",
      "Iteration:  180, Loss: 1281315.125000\n",
      "Iteration:  190, Loss: 1282041.625000\n",
      "Iteration:  200, Loss: 1282415.750000\n",
      "Iteration:  210, Loss: 540390.437500\n",
      "Iteration:  220, Loss: 535280.187500\n",
      "Iteration:  230, Loss: 530665.687500\n",
      "Iteration:  240, Loss: 528262.125000\n",
      "Iteration:  250, Loss: 526307.000000\n",
      "Iteration:  260, Loss: 524606.062500\n",
      "Iteration:  270, Loss: 523257.062500\n",
      "Iteration:  280, Loss: 522266.250000\n",
      "Iteration:  290, Loss: 521347.812500\n",
      "Iteration:  300, Loss: 520416.781250\n",
      "Elapsed time: 206.38s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1268658.0\n",
      "Iteration:   10, Loss: 366930.343750\n",
      "Iteration:   20, Loss: 260235.109375\n",
      "Iteration:   30, Loss: 221593.718750\n",
      "Iteration:   40, Loss: 205188.609375\n",
      "Iteration:   50, Loss: 201313.437500\n",
      "Iteration:   60, Loss: 199503.875000\n",
      "Iteration:   70, Loss: 198865.265625\n",
      "Iteration:   80, Loss: 198624.375000\n",
      "Iteration:   90, Loss: 198555.125000\n",
      "Iteration:  100, Loss: 198516.328125\n",
      "Iteration:  110, Loss: 298417.718750\n",
      "Iteration:  120, Loss: 298086.343750\n",
      "Iteration:  130, Loss: 297967.875000\n",
      "Iteration:  140, Loss: 297941.250000\n",
      "Iteration:  150, Loss: 297913.656250\n",
      "Iteration:  160, Loss: 297911.531250\n",
      "Iteration:  170, Loss: 297925.781250\n",
      "Iteration:  180, Loss: 297944.562500\n",
      "Iteration:  190, Loss: 297953.843750\n",
      "Iteration:  200, Loss: 297984.625000\n",
      "Iteration:  210, Loss: 99303.289062\n",
      "Iteration:  220, Loss: 99263.476562\n",
      "Iteration:  230, Loss: 99256.039062\n",
      "Iteration:  240, Loss: 99252.515625\n",
      "Iteration:  250, Loss: 99251.695312\n",
      "Iteration:  260, Loss: 99251.171875\n",
      "Iteration:  270, Loss: 99251.132812\n",
      "Iteration:  280, Loss: 99250.992188\n",
      "Iteration:  290, Loss: 99250.945312\n",
      "Iteration:  300, Loss: 99250.984375\n",
      "Elapsed time: 0:00:12.565124\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6371945.5\n",
      "Iteration:   10, Loss: 1830657.000000\n",
      "Iteration:   20, Loss: 1300570.625000\n",
      "Iteration:   30, Loss: 1106398.125000\n",
      "Iteration:   40, Loss: 1015851.000000\n",
      "Iteration:   50, Loss: 1000235.062500\n",
      "Iteration:   60, Loss: 992898.375000\n",
      "Iteration:   70, Loss: 990650.562500\n",
      "Iteration:   80, Loss: 989843.937500\n",
      "Iteration:   90, Loss: 989589.687500\n",
      "Iteration:  100, Loss: 989462.937500\n",
      "Iteration:  110, Loss: 1473723.375000\n",
      "Iteration:  120, Loss: 1470752.875000\n",
      "Iteration:  130, Loss: 1469980.625000\n",
      "Iteration:  140, Loss: 1469817.625000\n",
      "Iteration:  150, Loss: 1469677.250000\n",
      "Iteration:  160, Loss: 1469694.250000\n",
      "Iteration:  170, Loss: 1469783.500000\n",
      "Iteration:  180, Loss: 1469922.125000\n",
      "Iteration:  190, Loss: 1469966.375000\n",
      "Iteration:  200, Loss: 1470167.375000\n",
      "Iteration:  210, Loss: 494861.937500\n",
      "Iteration:  220, Loss: 494737.500000\n",
      "Iteration:  230, Loss: 494717.218750\n",
      "Iteration:  240, Loss: 494705.250000\n",
      "Iteration:  250, Loss: 494700.812500\n",
      "Iteration:  260, Loss: 494698.875000\n",
      "Iteration:  270, Loss: 494699.781250\n",
      "Iteration:  280, Loss: 494700.968750\n",
      "Iteration:  290, Loss: 494701.687500\n",
      "Iteration:  300, Loss: 494701.312500\n",
      "Elapsed time: 0:00:53.488153\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2403232.000000\n",
      "Iteration:   20, Loss: 2148636.000000\n",
      "Iteration:   30, Loss: 2015106.000000\n",
      "Iteration:   40, Loss: 1911030.250000\n",
      "Iteration:   50, Loss: 1811591.625000\n",
      "Iteration:   60, Loss: 1706247.625000\n",
      "Iteration:   70, Loss: 1588037.500000\n",
      "Iteration:   80, Loss: 1447791.500000\n",
      "Iteration:   90, Loss: 1271474.250000\n",
      "Iteration:  100, Loss: 1005498.250000\n",
      "Iteration:  110, Loss: 1315871.375000\n",
      "Iteration:  120, Loss: 1293339.750000\n",
      "Iteration:  130, Loss: 1283857.000000\n",
      "Iteration:  140, Loss: 1280737.875000\n",
      "Iteration:  150, Loss: 1280162.375000\n",
      "Iteration:  160, Loss: 1280158.500000\n",
      "Iteration:  170, Loss: 1280127.500000\n",
      "Iteration:  180, Loss: 1280724.750000\n",
      "Iteration:  190, Loss: 1281292.000000\n",
      "Iteration:  200, Loss: 1281537.875000\n",
      "Iteration:  210, Loss: 540056.562500\n",
      "Iteration:  220, Loss: 534958.250000\n",
      "Iteration:  230, Loss: 530429.125000\n",
      "Iteration:  240, Loss: 527984.937500\n",
      "Iteration:  250, Loss: 525939.312500\n",
      "Iteration:  260, Loss: 524128.875000\n",
      "Iteration:  270, Loss: 522770.937500\n",
      "Iteration:  280, Loss: 521848.437500\n",
      "Iteration:  290, Loss: 521029.375000\n",
      "Iteration:  300, Loss: 520177.562500\n",
      "Elapsed time: 205.92s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1269011.5\n",
      "Iteration:   10, Loss: 370999.812500\n",
      "Iteration:   20, Loss: 262723.250000\n",
      "Iteration:   30, Loss: 221981.625000\n",
      "Iteration:   40, Loss: 204879.859375\n",
      "Iteration:   50, Loss: 201338.828125\n",
      "Iteration:   60, Loss: 199488.265625\n",
      "Iteration:   70, Loss: 198840.343750\n",
      "Iteration:   80, Loss: 198601.343750\n",
      "Iteration:   90, Loss: 198531.671875\n",
      "Iteration:  100, Loss: 198490.781250\n",
      "Iteration:  110, Loss: 298377.281250\n",
      "Iteration:  120, Loss: 298054.750000\n",
      "Iteration:  130, Loss: 297950.406250\n",
      "Iteration:  140, Loss: 297909.343750\n",
      "Iteration:  150, Loss: 297868.656250\n",
      "Iteration:  160, Loss: 297869.000000\n",
      "Iteration:  170, Loss: 297881.937500\n",
      "Iteration:  180, Loss: 297901.593750\n",
      "Iteration:  190, Loss: 297917.718750\n",
      "Iteration:  200, Loss: 297945.187500\n",
      "Iteration:  210, Loss: 99289.242188\n",
      "Iteration:  220, Loss: 99250.710938\n",
      "Iteration:  230, Loss: 99244.000000\n",
      "Iteration:  240, Loss: 99240.937500\n",
      "Iteration:  250, Loss: 99239.695312\n",
      "Iteration:  260, Loss: 99239.156250\n",
      "Iteration:  270, Loss: 99238.867188\n",
      "Iteration:  280, Loss: 99238.812500\n",
      "Iteration:  290, Loss: 99238.859375\n",
      "Iteration:  300, Loss: 99238.859375\n",
      "Elapsed time: 0:00:12.028619\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6370287.0\n",
      "Iteration:   10, Loss: 1849968.625000\n",
      "Iteration:   20, Loss: 1312788.875000\n",
      "Iteration:   30, Loss: 1108373.125000\n",
      "Iteration:   40, Loss: 1014279.875000\n",
      "Iteration:   50, Loss: 999893.625000\n",
      "Iteration:   60, Loss: 992516.187500\n",
      "Iteration:   70, Loss: 990189.687500\n",
      "Iteration:   80, Loss: 989407.687500\n",
      "Iteration:   90, Loss: 989169.875000\n",
      "Iteration:  100, Loss: 989022.687500\n",
      "Iteration:  110, Loss: 1472904.625000\n",
      "Iteration:  120, Loss: 1469990.250000\n",
      "Iteration:  130, Loss: 1469268.125000\n",
      "Iteration:  140, Loss: 1469077.125000\n",
      "Iteration:  150, Loss: 1468941.625000\n",
      "Iteration:  160, Loss: 1468952.625000\n",
      "Iteration:  170, Loss: 1468993.750000\n",
      "Iteration:  180, Loss: 1469094.250000\n",
      "Iteration:  190, Loss: 1469143.375000\n",
      "Iteration:  200, Loss: 1469313.125000\n",
      "Iteration:  210, Loss: 494638.375000\n",
      "Iteration:  220, Loss: 494517.875000\n",
      "Iteration:  230, Loss: 494495.250000\n",
      "Iteration:  240, Loss: 494484.406250\n",
      "Iteration:  250, Loss: 494480.437500\n",
      "Iteration:  260, Loss: 494478.437500\n",
      "Iteration:  270, Loss: 494479.906250\n",
      "Iteration:  280, Loss: 494479.750000\n",
      "Iteration:  290, Loss: 494479.531250\n",
      "Iteration:  300, Loss: 494479.187500\n",
      "Elapsed time: 0:00:54.021241\n"
     ]
    }
   ],
   "source": [
    "## pacmap과 isolation forest 1차\n",
    "hhmm = 3\n",
    "what_val = superior_var2\n",
    "for num in range(hhmm):\n",
    "    embedding_1 = pacmap.PaCMAP(n_components=len(what_val), n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 300, verbose = True)\n",
    "    pacmac_train_1 = embedding_1.fit_transform(np.array(train_df.iloc[:,what_val]), init=\"pca\")\n",
    "    pacmac_val_1 = embedding_1.transform(np.array(val_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n",
    "    pacmac_test_1 = embedding_1.transform(np.array(test_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n",
    "\n",
    "    pac_model_1 = IsolationForest(n_estimators=300, contamination=0.00121, verbose=0)\n",
    "    pac_model_1.fit(pacmac_train_1[:,[1,2,3]])\n",
    "\n",
    "    if num == 0:\n",
    "        train_pred_set_1 = pac_model_1.predict(pacmac_train_1[:,[1,2,3]]) # model prediction\n",
    "        train_pred_set_1 = get_pred_label(train_pred_set_1)\n",
    "\n",
    "        val_pred_set_1 = pac_model_1.predict(pacmac_val_1[:,[1,2,3]]) # model prediction\n",
    "        val_pred_set_1 = get_pred_label(val_pred_set_1)\n",
    "\n",
    "        test_pred_set_1 = pac_model_1.predict(pacmac_test_1[:,[1,2,3]]) # model prediction\n",
    "        test_pred_set_1 = get_pred_label(test_pred_set_1)\n",
    "    else:\n",
    "        train_pred_1 = pac_model_1.predict(pacmac_train_1[:,[1,2,3]]) # model prediction\n",
    "        train_pred_1 = get_pred_label(train_pred_1)\n",
    "        train_pred_set_1 = train_pred_set_1 + train_pred_1\n",
    "\n",
    "        val_pred_1 = pac_model_1.predict(pacmac_val_1[:,[1,2,3]]) # model prediction\n",
    "        val_pred_1 = get_pred_label(val_pred_1)\n",
    "        val_pred_set_1 = val_pred_set_1 + val_pred_1\n",
    "\n",
    "        test_pred_1 = pac_model_1.predict(pacmac_test_1[:,[1,2,3]]) # model prediction\n",
    "        test_pred_1 = get_pred_label(test_pred_1)\n",
    "        test_pred_set_1 = test_pred_set_1 + test_pred_1\n",
    "\n",
    "train_pred_set_1 = train_pred_set_1/hhmm # score 임\n",
    "val_pred_set_1 = val_pred_set_1/hhmm\n",
    "test_pred_set_1 = test_pred_set_1/hhmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnfB0EBCb8K9"
   },
   "source": [
    "## 평가지표 출력\n",
    "\n",
    "- macro F1 score\n",
    "- 각 클래스의 정밀도, 재현율, F1 Score, 지원 개수\n",
    "- Confusion Matrix 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F4P1p5YVoc-s",
    "outputId": "976fb844-9250-4ae7-9e04-9b54d8b31c7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.9209734995691702]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28432\n",
      "           1       0.89      0.80      0.84        30\n",
      "\n",
      "    accuracy                           1.00     28462\n",
      "   macro avg       0.94      0.90      0.92     28462\n",
      "weighted avg       1.00      1.00      1.00     28462\n",
      "\n",
      "[[28429     3]\n",
      " [    6    24]]\n"
     ]
    }
   ],
   "source": [
    "## pacmap과 isolation forest 1차\n",
    "val_score_1 = f1_score(ori_val_df['Class'], np.round(val_pred_set_1), average='macro')\n",
    "\n",
    "print(f'Validation F1 Score : [{val_score_1}]')\n",
    "print(classification_report(ori_val_df['Class'], np.round(val_pred_set_1)))\n",
    "print(confusion_matrix(ori_val_df['Class'], np.round(val_pred_set_1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWld2-ydcTU4"
   },
   "source": [
    "## 예측결과 저장\n",
    "\n",
    "- 데이터 프레임 생성 후 원본데이터와 예측 클래스 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3rvSp1cCDcAR"
   },
   "outputs": [],
   "source": [
    "## pacmap과 isolation forest 1차\n",
    "chujung_train_1 = pd.DataFrame({'Class':np.round(train_pred_set_1)})\n",
    "chujung_val_1 = pd.DataFrame({'Class':np.round(val_pred_set_1)})\n",
    "chujung_test_1 = pd.DataFrame({'Class':np.round(test_pred_set_1)})\n",
    "\n",
    "result_train_1 = pd.concat([train_df,chujung_train_1], axis=1)\n",
    "result_val_1 = pd.concat([val_df,chujung_val_1], axis=1)\n",
    "result_test_1 = pd.concat([test_df,chujung_test_1], axis=1)\n",
    "\n",
    "result_train_1.to_csv('result_train_1.csv', index=False)\n",
    "result_val_1.to_csv('result_val_1.csv', index=False)\n",
    "result_test_1.to_csv('result_test_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WSUXG2UYa6vo"
   },
   "outputs": [],
   "source": [
    "## pacmap과 isolation forest 1차\n",
    "train_pred_set_1 = np.array(pd.read_csv('result_train_1.csv')['Class'])\n",
    "val_pred_set_1 = np.array(pd.read_csv('result_val_1.csv')['Class'])\n",
    "test_pred_set_1 = np.array(pd.read_csv('result_test_1.csv')['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhAM8EbO5an4"
   },
   "source": [
    "## take 2 : 변수 추가선택\n",
    "\n",
    "1. Inlier와 Outlier의 최대값 비교:\n",
    "\n",
    "- Inlier와 Outlier의 최대값을 비교하여\n",
    "- Inlier의 최대값이 Outlier의 최대값보다 작으면서,\n",
    "- Inlier의 최소값이 Outlier의 최소값보다 큰 변수를 식별\n",
    "- 이는 Inlier와 Outlier의 분포가 겹치지만, 서로 구별할 수 있는 변수를 찾는 데 도움\n",
    "\n",
    "2. Inlier의 분산과 Outlier의 분산 비교:\n",
    "- Inlier와 Outlier의 각 변수의 분산 비율을 계산하여,\n",
    "- 이를 기준으로 변수를 선택.\n",
    "- 분산 비율이 높을수록 해당 변수가 Inlier와 Outlier를 구별하는 데 더 도움이 될 것으로 판단\n",
    "\n",
    "3. Wilcoxon Rank Sum Test 수행:\n",
    "- 각 변수에 대해 Inlier와 Outlier를 나누고, 각 그룹의 중앙값과의 차이를 검정\n",
    "\n",
    "4. 교집합을 구한뒤 최종변수 선택\n",
    "\n",
    "\n",
    "old_born_idx:\n",
    "\n",
    "Inlier와 Outlier의 분산 비율을 기준으로\n",
    "\n",
    "left_born_idx:\n",
    "\n",
    "Inlier와 Outlier의 최대값 비교를 기준으로\n",
    "\n",
    "right_born_idx:\n",
    "\n",
    "Inlier와 Outlier의 최소값 비교를 기준으로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRX6ILqXa0c-",
    "outputId": "98e18860-1d4d-412a-bb66-6a490a5ee2f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 5, 12, 14, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 29] inside_in_inlier\n",
      "0 0.056611459159604575\n",
      "4 0.04223885928524464\n",
      "5 0.7704461525574464\n",
      "12 0.7649192536979115\n",
      "14 0.5924508408804137\n",
      "18 0.20398714517248465\n",
      "19 0.37151998169343714\n",
      "20 0.06681438471306114\n",
      "21 0.47076015284507583\n",
      "22 0.5165263768776028\n",
      "23 1.3711184736256365\n",
      "24 0.46796859661491885\n",
      "25 0.7132699705783758\n",
      "26 0.03718584298750135\n",
      "27 0.2518467335386625\n",
      "29 0.7823436145492879\n",
      "[23, 29, 5, 12, 25]\n",
      "[25, 21, 23, 24, 20] left_born_idx\n",
      "[5, 0, 24, 23, 29] right_born_idx\n",
      "[23] right_born_idx\n"
     ]
    }
   ],
   "source": [
    "## pacmap과 isolation forest 2차 이용\n",
    "inside_in_inlier= []\n",
    "\n",
    "for what_val in range(30):\n",
    "    if ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)].max()[what_val] <  ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)].max()[what_val]:\n",
    "        if ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)].min()[what_val] >  ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)].min()[what_val]:\n",
    "            inside_in_inlier.append(what_val)\n",
    "\n",
    "print(inside_in_inlier,'inside_in_inlier')\n",
    "\n",
    "var_chai = []\n",
    "for what_val in inside_in_inlier:\n",
    "    print(what_val , ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],what_val].var()/ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],what_val].var())\n",
    "    var_chai.append(ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],what_val].var()/ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],what_val].var())\n",
    "\n",
    "old_born_idx = np.argsort((-1)*np.array(var_chai))[:5]\n",
    "old_born_idx = np.array(inside_in_inlier)[list(old_born_idx)]\n",
    "old_born_idx = list(old_born_idx)\n",
    "print(old_born_idx,'old_born_idx')\n",
    "\n",
    "left_side_l = []\n",
    "right_side_l = []\n",
    "for jjkk in inside_in_inlier:\n",
    "    the_one_the_one = ori_val_df.iloc[np.where(ori_val_df['Class'] == 1)[0],jjkk]\n",
    "    median_the_one = the_one_the_one.median()\n",
    "    the_zero_the_zero = ori_val_df.iloc[np.where(ori_val_df['Class'] == 0)[0],jjkk]\n",
    "    left_zero_val_df = the_zero_the_zero.iloc[np.where(the_zero_the_zero < median_the_one)[0]]\n",
    "    right_zero_val_df = the_zero_the_zero.iloc[np.where(the_zero_the_zero > median_the_one)[0]]\n",
    "    left_ppp = (ranksums(left_zero_val_df, the_one_the_one).pvalue)\n",
    "    right_ppp = (ranksums(right_zero_val_df, the_one_the_one).pvalue)\n",
    "\n",
    "    left_side_l.append((left_ppp*1000)*((left_zero_val_df.var())))\n",
    "    right_side_l.append((right_ppp*1000)*((right_zero_val_df.var())))\n",
    "\n",
    "left_born_idx = np.argsort(np.array(left_side_l))[:5]\n",
    "left_born_idx = np.array(inside_in_inlier)[list(left_born_idx)]\n",
    "left_born_idx = list(left_born_idx)\n",
    "\n",
    "right_born_idx = np.argsort(np.array(right_side_l))[:5]\n",
    "right_born_idx = np.array(inside_in_inlier)[list(right_born_idx)]\n",
    "right_born_idx = list(right_born_idx)\n",
    "\n",
    "print(left_born_idx,'left_born_idx')\n",
    "print(right_born_idx,'right_born_idx')\n",
    "\n",
    "dhk_add_list = list(set(old_born_idx) & set(left_born_idx) & set(right_born_idx))\n",
    "print(dhk_add_list,'dhk_add_list')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nw7wdNha69Gv"
   },
   "source": [
    "## 학습 및 예측 (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tDnZmwISEkS6",
    "outputId": "9c2e920e-54b4-4e80-b82b-bba6a9372e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2462137.500000\n",
      "Iteration:   20, Loss: 2168929.250000\n",
      "Iteration:   30, Loss: 2027623.250000\n",
      "Iteration:   40, Loss: 1920732.875000\n",
      "Iteration:   50, Loss: 1820652.000000\n",
      "Iteration:   60, Loss: 1715430.750000\n",
      "Iteration:   70, Loss: 1597743.000000\n",
      "Iteration:   80, Loss: 1458857.750000\n",
      "Iteration:   90, Loss: 1283630.000000\n",
      "Iteration:  100, Loss: 1020191.000000\n",
      "Iteration:  110, Loss: 1338937.500000\n",
      "Iteration:  120, Loss: 1314367.125000\n",
      "Iteration:  130, Loss: 1302863.500000\n",
      "Iteration:  140, Loss: 1298772.000000\n",
      "Iteration:  150, Loss: 1297882.000000\n",
      "Iteration:  160, Loss: 1297913.125000\n",
      "Iteration:  170, Loss: 1297815.000000\n",
      "Iteration:  180, Loss: 1298655.375000\n",
      "Iteration:  190, Loss: 1299703.625000\n",
      "Iteration:  200, Loss: 1300132.750000\n",
      "Iteration:  210, Loss: 549864.125000\n",
      "Iteration:  220, Loss: 543812.437500\n",
      "Iteration:  230, Loss: 539169.750000\n",
      "Iteration:  240, Loss: 536514.500000\n",
      "Iteration:  250, Loss: 534334.875000\n",
      "Iteration:  260, Loss: 532576.375000\n",
      "Iteration:  270, Loss: 531296.375000\n",
      "Iteration:  280, Loss: 530313.437500\n",
      "Iteration:  290, Loss: 529480.375000\n",
      "Iteration:  300, Loss: 528576.250000\n",
      "Elapsed time: 145.43s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1275657.0\n",
      "Iteration:   10, Loss: 382932.656250\n",
      "Iteration:   20, Loss: 270233.062500\n",
      "Iteration:   30, Loss: 227101.984375\n",
      "Iteration:   40, Loss: 207905.000000\n",
      "Iteration:   50, Loss: 204352.406250\n",
      "Iteration:   60, Loss: 202520.031250\n",
      "Iteration:   70, Loss: 201785.484375\n",
      "Iteration:   80, Loss: 201572.031250\n",
      "Iteration:   90, Loss: 201485.578125\n",
      "Iteration:  100, Loss: 201445.968750\n",
      "Iteration:  110, Loss: 303070.187500\n",
      "Iteration:  120, Loss: 302613.593750\n",
      "Iteration:  130, Loss: 302478.093750\n",
      "Iteration:  140, Loss: 302426.031250\n",
      "Iteration:  150, Loss: 302393.312500\n",
      "Iteration:  160, Loss: 302376.000000\n",
      "Iteration:  170, Loss: 302384.250000\n",
      "Iteration:  180, Loss: 302405.031250\n",
      "Iteration:  190, Loss: 302418.593750\n",
      "Iteration:  200, Loss: 302458.093750\n",
      "Iteration:  210, Loss: 100789.179688\n",
      "Iteration:  220, Loss: 100733.562500\n",
      "Iteration:  230, Loss: 100723.726562\n",
      "Iteration:  240, Loss: 100719.226562\n",
      "Iteration:  250, Loss: 100718.132812\n",
      "Iteration:  260, Loss: 100717.554688\n",
      "Iteration:  270, Loss: 100717.312500\n",
      "Iteration:  280, Loss: 100717.296875\n",
      "Iteration:  290, Loss: 100717.125000\n",
      "Iteration:  300, Loss: 100717.156250\n",
      "Elapsed time: 0:00:14.456524\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6425687.5\n",
      "Iteration:   10, Loss: 1911631.375000\n",
      "Iteration:   20, Loss: 1352242.750000\n",
      "Iteration:   30, Loss: 1138094.625000\n",
      "Iteration:   40, Loss: 1030432.937500\n",
      "Iteration:   50, Loss: 1016577.125000\n",
      "Iteration:   60, Loss: 1009552.125000\n",
      "Iteration:   70, Loss: 1007140.312500\n",
      "Iteration:   80, Loss: 1006375.062500\n",
      "Iteration:   90, Loss: 1006075.125000\n",
      "Iteration:  100, Loss: 1005958.000000\n",
      "Iteration:  110, Loss: 1495409.375000\n",
      "Iteration:  120, Loss: 1491449.375000\n",
      "Iteration:  130, Loss: 1490470.875000\n",
      "Iteration:  140, Loss: 1490256.625000\n",
      "Iteration:  150, Loss: 1490102.500000\n",
      "Iteration:  160, Loss: 1490006.250000\n",
      "Iteration:  170, Loss: 1490048.125000\n",
      "Iteration:  180, Loss: 1490237.875000\n",
      "Iteration:  190, Loss: 1490372.875000\n",
      "Iteration:  200, Loss: 1490514.875000\n",
      "Iteration:  210, Loss: 503176.000000\n",
      "Iteration:  220, Loss: 503005.250000\n",
      "Iteration:  230, Loss: 502970.312500\n",
      "Iteration:  240, Loss: 502960.906250\n",
      "Iteration:  250, Loss: 502958.093750\n",
      "Iteration:  260, Loss: 502955.625000\n",
      "Iteration:  270, Loss: 502955.062500\n",
      "Iteration:  280, Loss: 502953.781250\n",
      "Iteration:  290, Loss: 502953.093750\n",
      "Iteration:  300, Loss: 502953.156250\n",
      "Elapsed time: 0:00:57.897012\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2462599.250000\n",
      "Iteration:   20, Loss: 2168465.500000\n",
      "Iteration:   30, Loss: 2027167.000000\n",
      "Iteration:   40, Loss: 1920457.250000\n",
      "Iteration:   50, Loss: 1820371.500000\n",
      "Iteration:   60, Loss: 1715108.875000\n",
      "Iteration:   70, Loss: 1597432.250000\n",
      "Iteration:   80, Loss: 1458564.000000\n",
      "Iteration:   90, Loss: 1283355.250000\n",
      "Iteration:  100, Loss: 1020124.125000\n",
      "Iteration:  110, Loss: 1339282.500000\n",
      "Iteration:  120, Loss: 1314343.500000\n",
      "Iteration:  130, Loss: 1302804.000000\n",
      "Iteration:  140, Loss: 1298691.000000\n",
      "Iteration:  150, Loss: 1297825.250000\n",
      "Iteration:  160, Loss: 1297707.625000\n",
      "Iteration:  170, Loss: 1297830.000000\n",
      "Iteration:  180, Loss: 1298527.875000\n",
      "Iteration:  190, Loss: 1299600.000000\n",
      "Iteration:  200, Loss: 1300067.750000\n",
      "Iteration:  210, Loss: 549884.750000\n",
      "Iteration:  220, Loss: 543773.250000\n",
      "Iteration:  230, Loss: 539152.937500\n",
      "Iteration:  240, Loss: 536530.687500\n",
      "Iteration:  250, Loss: 534367.062500\n",
      "Iteration:  260, Loss: 532622.000000\n",
      "Iteration:  270, Loss: 531288.500000\n",
      "Iteration:  280, Loss: 530312.375000\n",
      "Iteration:  290, Loss: 529479.375000\n",
      "Iteration:  300, Loss: 528604.562500\n",
      "Elapsed time: 145.54s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1275475.625\n",
      "Iteration:   10, Loss: 382438.406250\n",
      "Iteration:   20, Loss: 270487.250000\n",
      "Iteration:   30, Loss: 226757.000000\n",
      "Iteration:   40, Loss: 207730.718750\n",
      "Iteration:   50, Loss: 204172.781250\n",
      "Iteration:   60, Loss: 202340.484375\n",
      "Iteration:   70, Loss: 201603.765625\n",
      "Iteration:   80, Loss: 201391.187500\n",
      "Iteration:   90, Loss: 201304.531250\n",
      "Iteration:  100, Loss: 201266.328125\n",
      "Iteration:  110, Loss: 302795.187500\n",
      "Iteration:  120, Loss: 302336.968750\n",
      "Iteration:  130, Loss: 302195.093750\n",
      "Iteration:  140, Loss: 302149.031250\n",
      "Iteration:  150, Loss: 302108.656250\n",
      "Iteration:  160, Loss: 302095.562500\n",
      "Iteration:  170, Loss: 302107.593750\n",
      "Iteration:  180, Loss: 302121.875000\n",
      "Iteration:  190, Loss: 302146.531250\n",
      "Iteration:  200, Loss: 302182.125000\n",
      "Iteration:  210, Loss: 100697.929688\n",
      "Iteration:  220, Loss: 100642.617188\n",
      "Iteration:  230, Loss: 100632.601562\n",
      "Iteration:  240, Loss: 100628.117188\n",
      "Iteration:  250, Loss: 100626.953125\n",
      "Iteration:  260, Loss: 100627.031250\n",
      "Iteration:  270, Loss: 100626.484375\n",
      "Iteration:  280, Loss: 100626.304688\n",
      "Iteration:  290, Loss: 100626.171875\n",
      "Iteration:  300, Loss: 100626.226562\n",
      "Elapsed time: 0:00:13.371263\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6423646.0\n",
      "Iteration:   10, Loss: 1908511.250000\n",
      "Iteration:   20, Loss: 1353794.625000\n",
      "Iteration:   30, Loss: 1137035.625000\n",
      "Iteration:   40, Loss: 1030012.812500\n",
      "Iteration:   50, Loss: 1016087.250000\n",
      "Iteration:   60, Loss: 1009068.250000\n",
      "Iteration:   70, Loss: 1006633.625000\n",
      "Iteration:   80, Loss: 1005898.625000\n",
      "Iteration:   90, Loss: 1005586.812500\n",
      "Iteration:  100, Loss: 1005480.375000\n",
      "Iteration:  110, Loss: 1495027.000000\n",
      "Iteration:  120, Loss: 1490845.125000\n",
      "Iteration:  130, Loss: 1489801.375000\n",
      "Iteration:  140, Loss: 1489572.125000\n",
      "Iteration:  150, Loss: 1489386.875000\n",
      "Iteration:  160, Loss: 1489370.125000\n",
      "Iteration:  170, Loss: 1489455.250000\n",
      "Iteration:  180, Loss: 1489598.375000\n",
      "Iteration:  190, Loss: 1489727.500000\n",
      "Iteration:  200, Loss: 1489887.125000\n",
      "Iteration:  210, Loss: 502937.656250\n",
      "Iteration:  220, Loss: 502765.343750\n",
      "Iteration:  230, Loss: 502732.406250\n",
      "Iteration:  240, Loss: 502719.468750\n",
      "Iteration:  250, Loss: 502717.375000\n",
      "Iteration:  260, Loss: 502714.312500\n",
      "Iteration:  270, Loss: 502713.875000\n",
      "Iteration:  280, Loss: 502713.687500\n",
      "Iteration:  290, Loss: 502713.656250\n",
      "Iteration:  300, Loss: 502714.281250\n",
      "Elapsed time: 0:00:56.957468\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2461002.500000\n",
      "Iteration:   20, Loss: 2168646.250000\n",
      "Iteration:   30, Loss: 2027576.000000\n",
      "Iteration:   40, Loss: 1920792.250000\n",
      "Iteration:   50, Loss: 1820845.000000\n",
      "Iteration:   60, Loss: 1715522.250000\n",
      "Iteration:   70, Loss: 1597886.500000\n",
      "Iteration:   80, Loss: 1458952.000000\n",
      "Iteration:   90, Loss: 1283607.125000\n",
      "Iteration:  100, Loss: 1020331.062500\n",
      "Iteration:  110, Loss: 1339024.250000\n",
      "Iteration:  120, Loss: 1314257.500000\n",
      "Iteration:  130, Loss: 1302959.250000\n",
      "Iteration:  140, Loss: 1299015.875000\n",
      "Iteration:  150, Loss: 1298246.750000\n",
      "Iteration:  160, Loss: 1298197.750000\n",
      "Iteration:  170, Loss: 1298292.000000\n",
      "Iteration:  180, Loss: 1299057.125000\n",
      "Iteration:  190, Loss: 1299856.375000\n",
      "Iteration:  200, Loss: 1300395.375000\n",
      "Iteration:  210, Loss: 549932.625000\n",
      "Iteration:  220, Loss: 543837.937500\n",
      "Iteration:  230, Loss: 539176.437500\n",
      "Iteration:  240, Loss: 536515.312500\n",
      "Iteration:  250, Loss: 534361.000000\n",
      "Iteration:  260, Loss: 532656.875000\n",
      "Iteration:  270, Loss: 531346.750000\n",
      "Iteration:  280, Loss: 530323.937500\n",
      "Iteration:  290, Loss: 529459.812500\n",
      "Iteration:  300, Loss: 528563.375000\n",
      "Elapsed time: 143.03s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1276364.25\n",
      "Iteration:   10, Loss: 385097.750000\n",
      "Iteration:   20, Loss: 270060.093750\n",
      "Iteration:   30, Loss: 227086.296875\n",
      "Iteration:   40, Loss: 207751.671875\n",
      "Iteration:   50, Loss: 204218.000000\n",
      "Iteration:   60, Loss: 202368.125000\n",
      "Iteration:   70, Loss: 201642.437500\n",
      "Iteration:   80, Loss: 201432.062500\n",
      "Iteration:   90, Loss: 201345.000000\n",
      "Iteration:  100, Loss: 201305.453125\n",
      "Iteration:  110, Loss: 302852.593750\n",
      "Iteration:  120, Loss: 302401.656250\n",
      "Iteration:  130, Loss: 302259.593750\n",
      "Iteration:  140, Loss: 302214.031250\n",
      "Iteration:  150, Loss: 302188.562500\n",
      "Iteration:  160, Loss: 302168.343750\n",
      "Iteration:  170, Loss: 302162.937500\n",
      "Iteration:  180, Loss: 302191.437500\n",
      "Iteration:  190, Loss: 302199.281250\n",
      "Iteration:  200, Loss: 302232.250000\n",
      "Iteration:  210, Loss: 100716.640625\n",
      "Iteration:  220, Loss: 100662.757812\n",
      "Iteration:  230, Loss: 100652.546875\n",
      "Iteration:  240, Loss: 100648.312500\n",
      "Iteration:  250, Loss: 100647.359375\n",
      "Iteration:  260, Loss: 100646.835938\n",
      "Iteration:  270, Loss: 100646.812500\n",
      "Iteration:  280, Loss: 100646.601562\n",
      "Iteration:  290, Loss: 100646.585938\n",
      "Iteration:  300, Loss: 100646.648438\n",
      "Elapsed time: 0:00:13.300818\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6429903.5\n",
      "Iteration:   10, Loss: 1922223.875000\n",
      "Iteration:   20, Loss: 1351130.875000\n",
      "Iteration:   30, Loss: 1137902.125000\n",
      "Iteration:   40, Loss: 1029751.437500\n",
      "Iteration:   50, Loss: 1015735.562500\n",
      "Iteration:   60, Loss: 1008722.625000\n",
      "Iteration:   70, Loss: 1006229.312500\n",
      "Iteration:   80, Loss: 1005508.812500\n",
      "Iteration:   90, Loss: 1005211.125000\n",
      "Iteration:  100, Loss: 1005084.312500\n",
      "Iteration:  110, Loss: 1494435.875000\n",
      "Iteration:  120, Loss: 1490185.000000\n",
      "Iteration:  130, Loss: 1489172.375000\n",
      "Iteration:  140, Loss: 1488896.000000\n",
      "Iteration:  150, Loss: 1488748.750000\n",
      "Iteration:  160, Loss: 1488697.000000\n",
      "Iteration:  170, Loss: 1488791.500000\n",
      "Iteration:  180, Loss: 1488886.500000\n",
      "Iteration:  190, Loss: 1489000.125000\n",
      "Iteration:  200, Loss: 1489195.000000\n",
      "Iteration:  210, Loss: 502737.125000\n",
      "Iteration:  220, Loss: 502571.687500\n",
      "Iteration:  230, Loss: 502535.343750\n",
      "Iteration:  240, Loss: 502524.687500\n",
      "Iteration:  250, Loss: 502523.281250\n",
      "Iteration:  260, Loss: 502521.062500\n",
      "Iteration:  270, Loss: 502521.281250\n",
      "Iteration:  280, Loss: 502519.500000\n",
      "Iteration:  290, Loss: 502518.843750\n",
      "Iteration:  300, Loss: 502518.781250\n",
      "Elapsed time: 0:00:58.190187\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2460947.250000\n",
      "Iteration:   20, Loss: 2168403.500000\n",
      "Iteration:   30, Loss: 2027249.000000\n",
      "Iteration:   40, Loss: 1920551.125000\n",
      "Iteration:   50, Loss: 1820422.500000\n",
      "Iteration:   60, Loss: 1715182.500000\n",
      "Iteration:   70, Loss: 1597604.500000\n",
      "Iteration:   80, Loss: 1458757.750000\n",
      "Iteration:   90, Loss: 1283561.125000\n",
      "Iteration:  100, Loss: 1020281.000000\n",
      "Iteration:  110, Loss: 1339180.000000\n",
      "Iteration:  120, Loss: 1314440.875000\n",
      "Iteration:  130, Loss: 1302978.375000\n",
      "Iteration:  140, Loss: 1298911.125000\n",
      "Iteration:  150, Loss: 1297935.125000\n",
      "Iteration:  160, Loss: 1297968.125000\n",
      "Iteration:  170, Loss: 1297981.000000\n",
      "Iteration:  180, Loss: 1298662.500000\n",
      "Iteration:  190, Loss: 1299658.000000\n",
      "Iteration:  200, Loss: 1300208.875000\n",
      "Iteration:  210, Loss: 549957.187500\n",
      "Iteration:  220, Loss: 543843.437500\n",
      "Iteration:  230, Loss: 539216.875000\n",
      "Iteration:  240, Loss: 536572.062500\n",
      "Iteration:  250, Loss: 534392.937500\n",
      "Iteration:  260, Loss: 532661.625000\n",
      "Iteration:  270, Loss: 531340.250000\n",
      "Iteration:  280, Loss: 530355.312500\n",
      "Iteration:  290, Loss: 529510.750000\n",
      "Iteration:  300, Loss: 528644.500000\n",
      "Elapsed time: 142.53s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1275303.125\n",
      "Iteration:   10, Loss: 383594.562500\n",
      "Iteration:   20, Loss: 270311.312500\n",
      "Iteration:   30, Loss: 227068.093750\n",
      "Iteration:   40, Loss: 207791.031250\n",
      "Iteration:   50, Loss: 204183.484375\n",
      "Iteration:   60, Loss: 202384.953125\n",
      "Iteration:   70, Loss: 201626.000000\n",
      "Iteration:   80, Loss: 201423.062500\n",
      "Iteration:   90, Loss: 201333.312500\n",
      "Iteration:  100, Loss: 201292.453125\n",
      "Iteration:  110, Loss: 302854.718750\n",
      "Iteration:  120, Loss: 302394.937500\n",
      "Iteration:  130, Loss: 302243.375000\n",
      "Iteration:  140, Loss: 302193.000000\n",
      "Iteration:  150, Loss: 302146.437500\n",
      "Iteration:  160, Loss: 302133.187500\n",
      "Iteration:  170, Loss: 302143.468750\n",
      "Iteration:  180, Loss: 302176.218750\n",
      "Iteration:  190, Loss: 302200.000000\n",
      "Iteration:  200, Loss: 302223.531250\n",
      "Iteration:  210, Loss: 100710.101562\n",
      "Iteration:  220, Loss: 100656.828125\n",
      "Iteration:  230, Loss: 100646.312500\n",
      "Iteration:  240, Loss: 100641.601562\n",
      "Iteration:  250, Loss: 100640.117188\n",
      "Iteration:  260, Loss: 100639.687500\n",
      "Iteration:  270, Loss: 100639.343750\n",
      "Iteration:  280, Loss: 100639.187500\n",
      "Iteration:  290, Loss: 100639.289062\n",
      "Iteration:  300, Loss: 100639.187500\n",
      "Elapsed time: 0:00:13.489084\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6423640.0\n",
      "Iteration:   10, Loss: 1914055.500000\n",
      "Iteration:   20, Loss: 1353073.250000\n",
      "Iteration:   30, Loss: 1137745.875000\n",
      "Iteration:   40, Loss: 1029723.625000\n",
      "Iteration:   50, Loss: 1015742.250000\n",
      "Iteration:   60, Loss: 1008851.500000\n",
      "Iteration:   70, Loss: 1006348.375000\n",
      "Iteration:   80, Loss: 1005594.937500\n",
      "Iteration:   90, Loss: 1005290.562500\n",
      "Iteration:  100, Loss: 1005171.625000\n",
      "Iteration:  110, Loss: 1494499.000000\n",
      "Iteration:  120, Loss: 1490450.125000\n",
      "Iteration:  130, Loss: 1489386.375000\n",
      "Iteration:  140, Loss: 1489211.125000\n",
      "Iteration:  150, Loss: 1488995.750000\n",
      "Iteration:  160, Loss: 1488962.500000\n",
      "Iteration:  170, Loss: 1489017.750000\n",
      "Iteration:  180, Loss: 1489173.375000\n",
      "Iteration:  190, Loss: 1489283.250000\n",
      "Iteration:  200, Loss: 1489422.125000\n",
      "Iteration:  210, Loss: 502797.875000\n",
      "Iteration:  220, Loss: 502622.812500\n",
      "Iteration:  230, Loss: 502585.625000\n",
      "Iteration:  240, Loss: 502572.312500\n",
      "Iteration:  250, Loss: 502569.718750\n",
      "Iteration:  260, Loss: 502566.906250\n",
      "Iteration:  270, Loss: 502566.375000\n",
      "Iteration:  280, Loss: 502564.625000\n",
      "Iteration:  290, Loss: 502564.687500\n",
      "Iteration:  300, Loss: 502564.468750\n",
      "Elapsed time: 0:01:05.312637\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2461007.500000\n",
      "Iteration:   20, Loss: 2168304.500000\n",
      "Iteration:   30, Loss: 2027045.250000\n",
      "Iteration:   40, Loss: 1920464.375000\n",
      "Iteration:   50, Loss: 1820445.500000\n",
      "Iteration:   60, Loss: 1715101.750000\n",
      "Iteration:   70, Loss: 1597431.250000\n",
      "Iteration:   80, Loss: 1458554.500000\n",
      "Iteration:   90, Loss: 1283368.625000\n",
      "Iteration:  100, Loss: 1019986.937500\n",
      "Iteration:  110, Loss: 1339062.625000\n",
      "Iteration:  120, Loss: 1314395.250000\n",
      "Iteration:  130, Loss: 1302815.250000\n",
      "Iteration:  140, Loss: 1298573.625000\n",
      "Iteration:  150, Loss: 1297711.375000\n",
      "Iteration:  160, Loss: 1297572.250000\n",
      "Iteration:  170, Loss: 1297713.250000\n",
      "Iteration:  180, Loss: 1298533.500000\n",
      "Iteration:  190, Loss: 1299472.125000\n",
      "Iteration:  200, Loss: 1299809.000000\n",
      "Iteration:  210, Loss: 549832.500000\n",
      "Iteration:  220, Loss: 543754.812500\n",
      "Iteration:  230, Loss: 539148.687500\n",
      "Iteration:  240, Loss: 536509.125000\n",
      "Iteration:  250, Loss: 534337.437500\n",
      "Iteration:  260, Loss: 532568.937500\n",
      "Iteration:  270, Loss: 531260.562500\n",
      "Iteration:  280, Loss: 530308.000000\n",
      "Iteration:  290, Loss: 529448.812500\n",
      "Iteration:  300, Loss: 528571.812500\n",
      "Elapsed time: 145.20s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1275077.125\n",
      "Iteration:   10, Loss: 382587.562500\n",
      "Iteration:   20, Loss: 270560.156250\n",
      "Iteration:   30, Loss: 227242.093750\n",
      "Iteration:   40, Loss: 207798.546875\n",
      "Iteration:   50, Loss: 204319.453125\n",
      "Iteration:   60, Loss: 202466.687500\n",
      "Iteration:   70, Loss: 201737.546875\n",
      "Iteration:   80, Loss: 201523.500000\n",
      "Iteration:   90, Loss: 201434.765625\n",
      "Iteration:  100, Loss: 201397.156250\n",
      "Iteration:  110, Loss: 303011.656250\n",
      "Iteration:  120, Loss: 302546.281250\n",
      "Iteration:  130, Loss: 302420.500000\n",
      "Iteration:  140, Loss: 302356.406250\n",
      "Iteration:  150, Loss: 302318.406250\n",
      "Iteration:  160, Loss: 302293.718750\n",
      "Iteration:  170, Loss: 302299.281250\n",
      "Iteration:  180, Loss: 302323.843750\n",
      "Iteration:  190, Loss: 302331.906250\n",
      "Iteration:  200, Loss: 302379.406250\n",
      "Iteration:  210, Loss: 100762.382812\n",
      "Iteration:  220, Loss: 100707.625000\n",
      "Iteration:  230, Loss: 100697.078125\n",
      "Iteration:  240, Loss: 100693.796875\n",
      "Iteration:  250, Loss: 100692.507812\n",
      "Iteration:  260, Loss: 100692.210938\n",
      "Iteration:  270, Loss: 100692.156250\n",
      "Iteration:  280, Loss: 100691.804688\n",
      "Iteration:  290, Loss: 100691.890625\n",
      "Iteration:  300, Loss: 100692.000000\n",
      "Elapsed time: 0:00:13.312074\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6422731.5\n",
      "Iteration:   10, Loss: 1909674.125000\n",
      "Iteration:   20, Loss: 1353841.125000\n",
      "Iteration:   30, Loss: 1138696.625000\n",
      "Iteration:   40, Loss: 1029927.125000\n",
      "Iteration:   50, Loss: 1016495.000000\n",
      "Iteration:   60, Loss: 1009497.625000\n",
      "Iteration:   70, Loss: 1007015.312500\n",
      "Iteration:   80, Loss: 1006282.812500\n",
      "Iteration:   90, Loss: 1005968.312500\n",
      "Iteration:  100, Loss: 1005848.250000\n",
      "Iteration:  110, Loss: 1495345.625000\n",
      "Iteration:  120, Loss: 1491432.750000\n",
      "Iteration:  130, Loss: 1490455.375000\n",
      "Iteration:  140, Loss: 1490205.750000\n",
      "Iteration:  150, Loss: 1490006.875000\n",
      "Iteration:  160, Loss: 1489928.000000\n",
      "Iteration:  170, Loss: 1490062.750000\n",
      "Iteration:  180, Loss: 1490185.250000\n",
      "Iteration:  190, Loss: 1490309.375000\n",
      "Iteration:  200, Loss: 1490496.250000\n",
      "Iteration:  210, Loss: 503121.218750\n",
      "Iteration:  220, Loss: 502953.656250\n",
      "Iteration:  230, Loss: 502917.343750\n",
      "Iteration:  240, Loss: 502909.250000\n",
      "Iteration:  250, Loss: 502905.656250\n",
      "Iteration:  260, Loss: 502901.718750\n",
      "Iteration:  270, Loss: 502902.250000\n",
      "Iteration:  280, Loss: 502901.812500\n",
      "Iteration:  290, Loss: 502902.343750\n",
      "Iteration:  300, Loss: 502902.875000\n",
      "Elapsed time: 0:00:57.708504\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2461327.500000\n",
      "Iteration:   20, Loss: 2168466.250000\n",
      "Iteration:   30, Loss: 2027085.750000\n",
      "Iteration:   40, Loss: 1920331.250000\n",
      "Iteration:   50, Loss: 1820343.750000\n",
      "Iteration:   60, Loss: 1714982.750000\n",
      "Iteration:   70, Loss: 1597404.375000\n",
      "Iteration:   80, Loss: 1458634.125000\n",
      "Iteration:   90, Loss: 1283442.250000\n",
      "Iteration:  100, Loss: 1020231.750000\n",
      "Iteration:  110, Loss: 1339242.125000\n",
      "Iteration:  120, Loss: 1314468.750000\n",
      "Iteration:  130, Loss: 1302924.500000\n",
      "Iteration:  140, Loss: 1298789.000000\n",
      "Iteration:  150, Loss: 1297860.500000\n",
      "Iteration:  160, Loss: 1297890.375000\n",
      "Iteration:  170, Loss: 1297962.250000\n",
      "Iteration:  180, Loss: 1298625.625000\n",
      "Iteration:  190, Loss: 1299611.375000\n",
      "Iteration:  200, Loss: 1300081.500000\n",
      "Iteration:  210, Loss: 549950.875000\n",
      "Iteration:  220, Loss: 543865.000000\n",
      "Iteration:  230, Loss: 539253.000000\n",
      "Iteration:  240, Loss: 536621.750000\n",
      "Iteration:  250, Loss: 534459.812500\n",
      "Iteration:  260, Loss: 532661.250000\n",
      "Iteration:  270, Loss: 531341.625000\n",
      "Iteration:  280, Loss: 530353.062500\n",
      "Iteration:  290, Loss: 529530.375000\n",
      "Iteration:  300, Loss: 528654.125000\n",
      "Elapsed time: 143.87s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1275609.625\n",
      "Iteration:   10, Loss: 382962.000000\n",
      "Iteration:   20, Loss: 270697.062500\n",
      "Iteration:   30, Loss: 227141.234375\n",
      "Iteration:   40, Loss: 207829.921875\n",
      "Iteration:   50, Loss: 204306.828125\n",
      "Iteration:   60, Loss: 202484.843750\n",
      "Iteration:   70, Loss: 201744.218750\n",
      "Iteration:   80, Loss: 201534.296875\n",
      "Iteration:   90, Loss: 201444.718750\n",
      "Iteration:  100, Loss: 201406.750000\n",
      "Iteration:  110, Loss: 303026.812500\n",
      "Iteration:  120, Loss: 302576.968750\n",
      "Iteration:  130, Loss: 302426.718750\n",
      "Iteration:  140, Loss: 302368.468750\n",
      "Iteration:  150, Loss: 302329.218750\n",
      "Iteration:  160, Loss: 302297.343750\n",
      "Iteration:  170, Loss: 302314.656250\n",
      "Iteration:  180, Loss: 302345.156250\n",
      "Iteration:  190, Loss: 302372.625000\n",
      "Iteration:  200, Loss: 302397.125000\n",
      "Iteration:  210, Loss: 100767.851562\n",
      "Iteration:  220, Loss: 100712.632812\n",
      "Iteration:  230, Loss: 100702.375000\n",
      "Iteration:  240, Loss: 100698.101562\n",
      "Iteration:  250, Loss: 100696.976562\n",
      "Iteration:  260, Loss: 100696.468750\n",
      "Iteration:  270, Loss: 100696.226562\n",
      "Iteration:  280, Loss: 100696.015625\n",
      "Iteration:  290, Loss: 100696.093750\n",
      "Iteration:  300, Loss: 100696.101562\n",
      "Elapsed time: 0:00:13.507087\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6424994.5\n",
      "Iteration:   10, Loss: 1910376.000000\n",
      "Iteration:   20, Loss: 1354639.625000\n",
      "Iteration:   30, Loss: 1138142.750000\n",
      "Iteration:   40, Loss: 1030014.000000\n",
      "Iteration:   50, Loss: 1016183.875000\n",
      "Iteration:   60, Loss: 1009276.000000\n",
      "Iteration:   70, Loss: 1006821.000000\n",
      "Iteration:   80, Loss: 1006089.437500\n",
      "Iteration:   90, Loss: 1005781.312500\n",
      "Iteration:  100, Loss: 1005667.437500\n",
      "Iteration:  110, Loss: 1495201.875000\n",
      "Iteration:  120, Loss: 1491067.250000\n",
      "Iteration:  130, Loss: 1490024.750000\n",
      "Iteration:  140, Loss: 1489832.000000\n",
      "Iteration:  150, Loss: 1489583.875000\n",
      "Iteration:  160, Loss: 1489502.750000\n",
      "Iteration:  170, Loss: 1489591.625000\n",
      "Iteration:  180, Loss: 1489761.625000\n",
      "Iteration:  190, Loss: 1489911.125000\n",
      "Iteration:  200, Loss: 1490052.250000\n",
      "Iteration:  210, Loss: 503028.968750\n",
      "Iteration:  220, Loss: 502866.906250\n",
      "Iteration:  230, Loss: 502834.531250\n",
      "Iteration:  240, Loss: 502819.281250\n",
      "Iteration:  250, Loss: 502816.593750\n",
      "Iteration:  260, Loss: 502814.718750\n",
      "Iteration:  270, Loss: 502814.375000\n",
      "Iteration:  280, Loss: 502813.812500\n",
      "Iteration:  290, Loss: 502813.531250\n",
      "Iteration:  300, Loss: 502813.062500\n",
      "Elapsed time: 0:00:58.283148\n",
      "X is normalized\n",
      "PaCMAP(n_neighbors=26, n_MN=13, n_FP=52, distance=euclidean, lr=1.0, n_iters=300, apply_pca=True, opt_method='adam', verbose=True, intermediate=False, seed=None)\n",
      "Finding pairs\n",
      "Found nearest neighbor\n",
      "Calculated sigma\n",
      "Found scaled dist\n",
      "Pairs sampled successfully.\n",
      "((2959892, 2), (1479946, 2), (5919784, 2))\n",
      "Initial Loss: 3658542.0\n",
      "Iteration:   10, Loss: 2462183.250000\n",
      "Iteration:   20, Loss: 2168860.000000\n",
      "Iteration:   30, Loss: 2027394.500000\n",
      "Iteration:   40, Loss: 1920584.750000\n",
      "Iteration:   50, Loss: 1820483.000000\n",
      "Iteration:   60, Loss: 1715181.250000\n",
      "Iteration:   70, Loss: 1597534.125000\n",
      "Iteration:   80, Loss: 1458726.750000\n",
      "Iteration:   90, Loss: 1283488.375000\n",
      "Iteration:  100, Loss: 1020216.250000\n",
      "Iteration:  110, Loss: 1339171.875000\n",
      "Iteration:  120, Loss: 1314553.125000\n",
      "Iteration:  130, Loss: 1303138.875000\n",
      "Iteration:  140, Loss: 1299326.000000\n",
      "Iteration:  150, Loss: 1298690.875000\n",
      "Iteration:  160, Loss: 1298841.125000\n",
      "Iteration:  170, Loss: 1299105.625000\n",
      "Iteration:  180, Loss: 1299826.125000\n",
      "Iteration:  190, Loss: 1300869.250000\n",
      "Iteration:  200, Loss: 1301390.625000\n",
      "Iteration:  210, Loss: 550180.125000\n",
      "Iteration:  220, Loss: 544206.437500\n",
      "Iteration:  230, Loss: 539474.687500\n",
      "Iteration:  240, Loss: 536842.437500\n",
      "Iteration:  250, Loss: 534797.437500\n",
      "Iteration:  260, Loss: 533159.375000\n",
      "Iteration:  270, Loss: 531864.562500\n",
      "Iteration:  280, Loss: 530807.562500\n",
      "Iteration:  290, Loss: 529852.562500\n",
      "Iteration:  300, Loss: 528917.125000\n",
      "Elapsed time: 143.35s\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(740012, 2)\n",
      "Initial Loss: 1274333.125\n",
      "Iteration:   10, Loss: 380029.656250\n",
      "Iteration:   20, Loss: 267271.468750\n",
      "Iteration:   30, Loss: 226171.562500\n",
      "Iteration:   40, Loss: 208338.625000\n",
      "Iteration:   50, Loss: 204348.859375\n",
      "Iteration:   60, Loss: 202494.078125\n",
      "Iteration:   70, Loss: 201804.937500\n",
      "Iteration:   80, Loss: 201577.390625\n",
      "Iteration:   90, Loss: 201492.750000\n",
      "Iteration:  100, Loss: 201456.515625\n",
      "Iteration:  110, Loss: 303114.312500\n",
      "Iteration:  120, Loss: 302637.062500\n",
      "Iteration:  130, Loss: 302475.062500\n",
      "Iteration:  140, Loss: 302434.125000\n",
      "Iteration:  150, Loss: 302409.468750\n",
      "Iteration:  160, Loss: 302399.156250\n",
      "Iteration:  170, Loss: 302411.312500\n",
      "Iteration:  180, Loss: 302421.500000\n",
      "Iteration:  190, Loss: 302438.812500\n",
      "Iteration:  200, Loss: 302480.343750\n",
      "Iteration:  210, Loss: 100791.000000\n",
      "Iteration:  220, Loss: 100737.554688\n",
      "Iteration:  230, Loss: 100728.390625\n",
      "Iteration:  240, Loss: 100723.890625\n",
      "Iteration:  250, Loss: 100722.460938\n",
      "Iteration:  260, Loss: 100721.992188\n",
      "Iteration:  270, Loss: 100722.015625\n",
      "Iteration:  280, Loss: 100721.710938\n",
      "Iteration:  290, Loss: 100721.765625\n",
      "Iteration:  300, Loss: 100721.820312\n",
      "Elapsed time: 0:00:13.393165\n",
      "X is normalized.\n",
      "X is normalized.\n",
      "Found nearest neighbor\n",
      "Found scaled dist\n",
      "(3705078, 2)\n",
      "Initial Loss: 6423651.0\n",
      "Iteration:   10, Loss: 1898653.500000\n",
      "Iteration:   20, Loss: 1338848.000000\n",
      "Iteration:   30, Loss: 1134914.250000\n",
      "Iteration:   40, Loss: 1033128.937500\n",
      "Iteration:   50, Loss: 1017586.125000\n",
      "Iteration:   60, Loss: 1010407.437500\n",
      "Iteration:   70, Loss: 1008119.562500\n",
      "Iteration:   80, Loss: 1007339.875000\n",
      "Iteration:   90, Loss: 1007050.312500\n",
      "Iteration:  100, Loss: 1006930.812500\n",
      "Iteration:  110, Loss: 1497363.000000\n",
      "Iteration:  120, Loss: 1493034.625000\n",
      "Iteration:  130, Loss: 1491848.875000\n",
      "Iteration:  140, Loss: 1491588.875000\n",
      "Iteration:  150, Loss: 1491462.125000\n",
      "Iteration:  160, Loss: 1491519.875000\n",
      "Iteration:  170, Loss: 1491593.875000\n",
      "Iteration:  180, Loss: 1491739.750000\n",
      "Iteration:  190, Loss: 1491839.125000\n",
      "Iteration:  200, Loss: 1491973.625000\n",
      "Iteration:  210, Loss: 503672.468750\n",
      "Iteration:  220, Loss: 503503.906250\n",
      "Iteration:  230, Loss: 503473.625000\n",
      "Iteration:  240, Loss: 503460.093750\n",
      "Iteration:  250, Loss: 503456.312500\n",
      "Iteration:  260, Loss: 503452.375000\n",
      "Iteration:  270, Loss: 503452.406250\n",
      "Iteration:  280, Loss: 503452.937500\n",
      "Iteration:  290, Loss: 503452.375000\n",
      "Iteration:  300, Loss: 503451.812500\n",
      "Elapsed time: 0:00:57.995387\n"
     ]
    }
   ],
   "source": [
    "# pacmap과 isolation forest를 이용한 2차 예측\n",
    "hhmm = 7\n",
    "vlskffo_var = superior_var2 + dhk_add_list\n",
    "what_val = vlskffo_var\n",
    "\n",
    "for num in range(hhmm):\n",
    "    embedding_3 = pacmap.PaCMAP(n_components=len(what_val), n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters = 300, verbose = True)\n",
    "    pacmac_train_3 = embedding_3.fit_transform(np.array(train_df.iloc[:,what_val]), init=\"pca\")\n",
    "    pacmac_val_3 = embedding_3.transform(np.array(val_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n",
    "    pacmac_test_3 = embedding_3.transform(np.array(test_df.iloc[:,what_val]), basis=np.array(train_df.iloc[:,what_val]))\n",
    "\n",
    "    pac_model_3 = IsolationForest(n_estimators=300, contamination=0.00121, verbose=0)\n",
    "    pac_model_3.fit(pacmac_train_3[:,[1,2,3]])\n",
    "\n",
    "    if num == 0:\n",
    "        train_pred_set_3 = pac_model_3.predict(pacmac_train_3[:,[1,2,3]]) # model prediction\n",
    "        train_pred_set_3 = get_pred_label(train_pred_set_3)\n",
    "\n",
    "        val_pred_set_3 = pac_model_3.predict(pacmac_val_3[:,[1,2,3]]) # model prediction\n",
    "        val_pred_set_3 = get_pred_label(val_pred_set_3)\n",
    "\n",
    "        test_pred_set_3 = pac_model_3.predict(pacmac_test_3[:,[1,2,3]]) # model prediction\n",
    "        test_pred_set_3 = get_pred_label(test_pred_set_3)\n",
    "    else:\n",
    "        train_pred_3 = pac_model_3.predict(pacmac_train_3[:,[1,2,3]]) # model prediction\n",
    "        train_pred_3 = get_pred_label(train_pred_3)\n",
    "        train_pred_set_3 = train_pred_set_3 + train_pred_3\n",
    "\n",
    "        val_pred_3 = pac_model_3.predict(pacmac_val_3[:,[1,2,3]]) # model prediction\n",
    "        val_pred_3 = get_pred_label(val_pred_3)\n",
    "        val_pred_set_3 = val_pred_set_3 + val_pred_3\n",
    "\n",
    "        test_pred_3 = pac_model_3.predict(pacmac_test_3[:,[1,2,3]]) # model prediction\n",
    "        test_pred_3 = get_pred_label(test_pred_3)\n",
    "        test_pred_set_3 = test_pred_set_3 + test_pred_3\n",
    "\n",
    "train_pred_set_3 = train_pred_set_3/hhmm\n",
    "val_pred_set_3 = val_pred_set_3/hhmm\n",
    "test_pred_set_3 = test_pred_set_3/hhmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P_pRa36E9BIb",
    "outputId": "8ad24b1a-f3ae-4a29-b50a-4b19936b3708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score : [0.6963959681062595]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     28432\n",
      "           1       0.39      0.40      0.39        30\n",
      "\n",
      "    accuracy                           1.00     28462\n",
      "   macro avg       0.69      0.70      0.70     28462\n",
      "weighted avg       1.00      1.00      1.00     28462\n",
      "\n",
      "[[28413    19]\n",
      " [   18    12]]\n"
     ]
    }
   ],
   "source": [
    "## pacmap과 isolation forest 2차 예측\n",
    "val_score_3 = f1_score(ori_val_df['Class'], np.round(val_pred_set_3), average='macro')\n",
    "\n",
    "print(f'Validation F1 Score : [{val_score_3}]')\n",
    "print(classification_report(ori_val_df['Class'], np.round(val_pred_set_3)))\n",
    "print(confusion_matrix(ori_val_df['Class'], np.round(val_pred_set_3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "_h8Q3KX3FINe"
   },
   "outputs": [],
   "source": [
    "## pacmap과 isolation forest 2차 예측\n",
    "# 2차 저장\n",
    "chujung_train_2 = pd.DataFrame({'Class':np.round(train_pred_set_3)})\n",
    "chujung_val_2 = pd.DataFrame({'Class':np.round(val_pred_set_3)})\n",
    "chujung_test_2 = pd.DataFrame({'Class':np.round(test_pred_set_3)})\n",
    "\n",
    "result_train_2 = pd.concat([train_df,chujung_train_2], axis=1)\n",
    "result_val_2 = pd.concat([val_df,chujung_val_2], axis=1)\n",
    "result_test_2 = pd.concat([test_df,chujung_test_2], axis=1)\n",
    "\n",
    "result_train_2.to_csv('result_train_2.csv', index=False)\n",
    "result_val_2.to_csv('result_val_2.csv', index=False)\n",
    "result_test_2.to_csv('result_test_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "jRvSPekcWYhu"
   },
   "outputs": [],
   "source": [
    "## pacmap과 isolation forest 2차 예측\n",
    "# 2차 불러오기 및 outlier된 dataset 모음\n",
    "train_pred_set_3 = np.array(pd.read_csv('result_train_2.csv')['Class'])\n",
    "val_pred_set_3 = np.array(pd.read_csv('result_val_2.csv')['Class'])\n",
    "test_pred_set_3 = np.array(pd.read_csv('result_test_2.csv')['Class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GT5c2wrNcLQY"
   },
   "source": [
    "![image.png](data:image/png;base64,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)\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhCE1HXVcM4p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7kmXNKH7EuC"
   },
   "source": [
    "### Outlier 중 가짜판별하기\n",
    "\n",
    "- 각 변수에 대해 Wilcoxon 순위 합 검정을 사용하여 이상치로 판별된 그룹(클래스 1)과 정상 그룹(클래스 0) 간의 차이를 평가 후, p-value가 0.05보다 큰 변수들을 선택\n",
    "\n",
    "- 선택된 변수들에 대해 정상 그룹과 이상치 그룹의 분산 비율을 계산\n",
    "\n",
    "- 분산 비율이 작은 순서대로 변수들을 정렬하고, 그 중 가장 작은 5개의 인덱스를 선택하여 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "rzRd6lZ3FOvF"
   },
   "outputs": [],
   "source": [
    "## outlier 중 가짜 판별(1)\n",
    "# 변수 선택\n",
    "\n",
    "rrr_df = []\n",
    "for what_val in range(30):\n",
    "    rrr_df.append(ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],what_val],one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],what_val]).pvalue)\n",
    "\n",
    "rrr_idx = np.where(np.array(rrr_df) > 0.05)[0]\n",
    "\n",
    "qqq_df = []\n",
    "for what_val in rrr_idx:\n",
    "    qqq_df.append(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],what_val].var()/one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],what_val].var())\n",
    "\n",
    "new_born_idx = np.argsort(qqq_df)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxgqFL_Q3UXT",
    "outputId": "75f18fa6-e7a2-4b89-fa04-88550b2061da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 0 2 3 7]\n"
     ]
    }
   ],
   "source": [
    "print(new_born_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18Ryb5lq7bYB"
   },
   "source": [
    "## take 3\n",
    "\n",
    "1. Kernel PCA를 사용하여 저차원 공간으로 변환\n",
    "\n",
    "2. 판별 (Discrimination):\n",
    "\n",
    "- Pacmap을 사용하여 데이터를 시각화하고, 판별 기준을 설정\n",
    "\n",
    "3. 이상치 판별\n",
    "\n",
    "- pacmap의 결과를 기준으로 실제 이상치와 가짜 이상치를 분류\n",
    "\n",
    "- 중위수 차이를 비교하여 가짜 이상치를 판별\n",
    "\n",
    "\n",
    "4. 결과 저장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "OFPFE1YA5AzO",
    "outputId": "d43ca04c-3ba6-4f31-8250-9fd4f2c03204"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pred_set_1010' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-33ad4868103c>\u001b[0m in \u001b[0;36m<cell line: 189>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0mtrain_pred_set_1010\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_pred_set_1010\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhhmm\u001b[0m\u001b[0;34m-\u001b[0m \u001b[0mskip_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0mval_pred_set_1010\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_pred_set_1010\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhhmm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mskip_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m \u001b[0mtest_pred_set_1010\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_pred_set_1010\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhhmm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mskip_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_pred_set_1010' is not defined"
     ]
    }
   ],
   "source": [
    "## outlier 중 가짜 판별(2)\n",
    "# 판별\n",
    "transformer = KernelPCA(n_components=5, kernel='rbf')\n",
    "hhhhh = new_born_idx\n",
    "can_sepearate = np.argsort(rrr_df)[0]\n",
    "\n",
    "X_transformed = transformer.fit_transform(one_train_df.iloc[:,hhhhh])\n",
    "X1_transformed = transformer.transform(one_val_df.iloc[:,hhhhh])\n",
    "X2_transformed = transformer.transform(one_test_df.iloc[:,hhhhh])\n",
    "\n",
    "# kernel pca 후 1에서 0 찾기 by pacmap 그후 판별\n",
    "hhmm = 201\n",
    "skip_count = 0\n",
    "\n",
    "contamin = 0.0725\n",
    "making_set = 0\n",
    "\n",
    "train_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_train_wow))})\n",
    "\n",
    "for num in range(hhmm):\n",
    "    embedding1010 = pacmap.PaCMAP(n_components=3, n_neighbors=None, MN_ratio=0.5, FP_ratio=2.0, num_iters=500, verbose=False)\n",
    "    pacmac_train1010 = embedding1010.fit_transform(X_transformed, init=\"pca\")\n",
    "    pacmac_val1010 = embedding1010.transform(X1_transformed, basis=X_transformed)\n",
    "    pacmac_test1010 = embedding1010.transform(X2_transformed, basis=X_transformed)\n",
    "\n",
    "    # 그냥 pca\n",
    "    pcawow = PCA(n_components = 3) # 주성분을 몇개로 할지 결정\n",
    "    pca_train_wow = pcawow.fit_transform(pacmac_train1010)\n",
    "    pca_train_wow = pd.DataFrame(data=pca_train_wow)\n",
    "\n",
    "    pca_val_wow = pcawow.transform(pacmac_val1010)\n",
    "    pca_val_wow = pd.DataFrame(data=pca_val_wow)\n",
    "\n",
    "    pca_test_wow = pcawow.transform(pacmac_test1010)\n",
    "    pca_test_wow = pd.DataFrame(data=pca_test_wow)\n",
    "\n",
    "    rlwns = int(contamin*len(pca_train_wow))\n",
    "    thtn = contamin*len(pca_train_wow) - rlwns\n",
    "\n",
    "    if making_set == 0:\n",
    "        max_part_zero = 0\n",
    "        max_part_one = 0\n",
    "        max_what = 100\n",
    "\n",
    "        # PC 축을 설정하고 좌 또는 우 극단값을 찾고 해당하는 중위값이 실제 outlier들의 중위값인지 체크\n",
    "        for jrj in range(6):\n",
    "            val_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n",
    "\n",
    "            if jrj % 2 == 0:\n",
    "                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nlargest(rlwns).iloc[(rlwns-1)]\n",
    "                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "                val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn) ))] = 0\n",
    "            else:\n",
    "                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n",
    "                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "                val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "            ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_set_1010['Class'] == 1)[0],can_sepearate]).pvalue\n",
    "            ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_set_1010['Class'] == 0)[0],can_sepearate]).pvalue\n",
    "\n",
    "            if np.isnan(ranksum_pval_ed_zero):\n",
    "                continue\n",
    "\n",
    "            if max_part_zero <= ranksum_pval_ed_zero:\n",
    "                max_part_zero = ranksum_pval_ed_zero\n",
    "            if max_part_one <= ranksum_pval_ed_one:\n",
    "                max_part_one = ranksum_pval_ed_one\n",
    "\n",
    "            if max_part_zero >= 0.5:\n",
    "                if max_part_one >= 0.5:\n",
    "                    max_what = jrj\n",
    "                    break\n",
    "\n",
    "        if max_what == 100:\n",
    "            skip_count += 1\n",
    "            continue\n",
    "        making_set += 10\n",
    "\n",
    "        # 체크 후 해당하는 축으로 가짜 outlier 판별\n",
    "        val_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nlargest(rlwns).iloc[(rlwns-1)]\n",
    "            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "            val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        else:\n",
    "            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n",
    "            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "            val_pred_set_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        train_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_train_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            train_pred_set_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "        else:\n",
    "            train_pred_set_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        test_pred_set_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_test_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            test_pred_set_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "        else:\n",
    "            test_pred_set_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "    else:\n",
    "        max_part_zero = 0\n",
    "        max_part_one = 0\n",
    "        max_what = 100\n",
    "\n",
    "        # PC 축을 설정하고 좌 또는 우 극단값을 찾고 해당하는 중위값이 실제 outlier들의 중위값인지 체크\n",
    "        for jrj in range(6):\n",
    "            val_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n",
    "\n",
    "            if jrj % 2 == 0:\n",
    "                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nlargest(rlwns).iloc[(rlwns-1)]\n",
    "                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "                val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn) ))] = 0\n",
    "            else:\n",
    "                rlwns_val = pca_train_wow.iloc[:,(jrj//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n",
    "                rlwns_val_Qkd = pca_train_wow.iloc[:,(jrj//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "                val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(jrj//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "            ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 1)[0],can_sepearate]).pvalue\n",
    "            ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 0)[0],can_sepearate]).pvalue\n",
    "\n",
    "            if np.isnan(ranksum_pval_ed_zero):\n",
    "                continue\n",
    "\n",
    "            if max_part_zero <= ranksum_pval_ed_zero:\n",
    "                max_part_zero = ranksum_pval_ed_zero\n",
    "            if max_part_one <= ranksum_pval_ed_one:\n",
    "                max_part_one = ranksum_pval_ed_one\n",
    "\n",
    "            if max_part_zero >= 0.5:\n",
    "                if max_part_one >= 0.5:\n",
    "                    max_what = jrj\n",
    "                    break\n",
    "\n",
    "        if max_what == 100:\n",
    "            skip_count += 1\n",
    "            continue\n",
    "\n",
    "        # 체크 후 해당하는 축으로 가짜 outlier 판별\n",
    "        val_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_val_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nlargest(rlwns).iloc[(rlwns-1)]\n",
    "            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nlargest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "            val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "        else:\n",
    "            rlwns_val = pca_train_wow.iloc[:,(max_what//2)].nsmallest(rlwns).iloc[(rlwns-1)]\n",
    "            rlwns_val_Qkd = pca_train_wow.iloc[:,(max_what//2)].nsmallest((rlwns+1)).iloc[(rlwns)]\n",
    "\n",
    "            val_pred_1010.iloc[np.where(pca_val_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        ranksum_pval_ed_one = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 1)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 1)[0],can_sepearate]).pvalue\n",
    "        ranksum_pval_ed_zero = ranksums(one_val_df.iloc[np.where(one_val_df['Class'] == 0)[0],can_sepearate],one_val_df.iloc[np.where(val_pred_1010['Class'] == 0)[0],can_sepearate]).pvalue\n",
    "\n",
    "        val_pred_set_1010 = val_pred_set_1010 + val_pred_1010\n",
    "\n",
    "        train_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_train_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            train_pred_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "        else:\n",
    "            train_pred_1010.iloc[np.where(pca_train_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        train_pred_set_1010 = train_pred_set_1010 + train_pred_1010\n",
    "\n",
    "        test_pred_1010 = pd.DataFrame({'Class':np.repeat(1,len(pca_test_wow))})\n",
    "\n",
    "        if max_what % 2 == 0:\n",
    "            test_pred_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] >= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "        else:\n",
    "            test_pred_1010.iloc[np.where(pca_test_wow.iloc[:,(max_what//2)] <= (rlwns_val_Qkd*thtn  + rlwns_val*(1-thtn)))] = 0\n",
    "\n",
    "        test_pred_set_1010 = test_pred_set_1010 + test_pred_1010\n",
    "\n",
    "train_pred_set_1010 = train_pred_set_1010/(hhmm- skip_count)\n",
    "val_pred_set_1010 = val_pred_set_1010/(hhmm-skip_count)\n",
    "test_pred_set_1010 = test_pred_set_1010/(hhmm-skip_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52xsbOCcgsay"
   },
   "source": [
    "## 평가 지표 출력 및 결과 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7t4tiU-QFwnQ"
   },
   "outputs": [],
   "source": [
    "## 최종 확인\n",
    "# voting해서 50% 이상 1이면 1로, 반대면 0으로\n",
    "final_one_train_df = one_train_df.iloc[np.where(np.round(train_pred_set_1010['Class']) == 1)[0]]\n",
    "final_one_val_df = one_val_df.iloc[np.where(np.round(val_pred_set_1010['Class']) == 1)[0]]\n",
    "final_one_test_df = one_test_df.iloc[np.where(np.round(test_pred_set_1010['Class']) == 1)[0]]\n",
    "\n",
    "# validation score 확인\n",
    "chujung_train2 = pd.DataFrame({'Class':np.repeat(0,len(train_df))})\n",
    "chujung_val2 = pd.DataFrame({'pre_Class':np.repeat(0,len(val_df))})\n",
    "chujung_test2 = pd.DataFrame({'Class':np.repeat(0,len(test_df))})\n",
    "\n",
    "chujung_train2.iloc[final_one_train_df.index,0] = 1\n",
    "chujung_val2.iloc[final_one_val_df.index,0] = 1\n",
    "chujung_test2.iloc[final_one_test_df.index,0] = 1\n",
    "\n",
    "sec_result_train = pd.concat([train_df,chujung_train2], axis=1)\n",
    "sec_result_val = pd.concat([val_df,chujung_val2], axis=1)\n",
    "sec_result_test = pd.concat([test_df,chujung_test2], axis=1)\n",
    "\n",
    "print('F1-score',f1_score(sec_result_val['pre_Class'], val_class, average='macro'))\n",
    "print(confusion_matrix(sec_result_val['pre_Class'], val_class))\n",
    "print(classification_report(sec_result_val['pre_Class'], val_class))\n",
    "\n",
    "# 저장\n",
    "submit = pd.read_csv('sample_submission.csv')\n",
    "submit['Class'] = 0\n",
    "\n",
    "submit.iloc[final_one_test_df.index,1] = 1\n",
    "submit.iloc[final_one_test_df.index,1]\n",
    "\n",
    "submit.to_csv('result_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
